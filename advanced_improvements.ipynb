{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Advanced Improvements for Loan Repayment Prediction\n",
    "\n",
    "**Goal:** Push AUC-ROC score beyond 0.92178 using advanced techniques\n",
    "\n",
    "This notebook implements:\n",
    "1. Advanced Feature Engineering\n",
    "2. Ensemble Methods (Stacking/Blending)\n",
    "3. Class Imbalance Handling\n",
    "4. Feature Selection\n",
    "5. Advanced Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Training set shape: (593994, 13)\n",
      "üìä Test set shape: (254569, 12)\n",
      "\n",
      "‚úÖ Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "print(f\"üìä Training set shape: {train.shape}\")\n",
    "print(f\"üìä Test set shape: {test.shape}\")\n",
    "print(f\"\\n‚úÖ Data loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ No duplicates found in training set\n",
      "‚úÖ No duplicates found in test set\n",
      "\n",
      "Final shapes:\n",
      "Train: (593994, 13)\n",
      "Test: (254569, 12)\n"
     ]
    }
   ],
   "source": [
    "# Create copies for cleaning\n",
    "train_clean = train.copy()\n",
    "test_clean = test.copy()\n",
    "\n",
    "# Check for duplicates\n",
    "train_duplicates = train_clean.duplicated().sum()\n",
    "test_duplicates = test_clean.duplicated().sum()\n",
    "\n",
    "if train_duplicates > 0:\n",
    "    train_clean = train_clean.drop_duplicates()\n",
    "    print(f\"‚úÖ Removed {train_duplicates} duplicates from training set\")\n",
    "else:\n",
    "    print(\"‚úÖ No duplicates found in training set\")\n",
    "\n",
    "if test_duplicates > 0:\n",
    "    test_clean = test_clean.drop_duplicates()\n",
    "    print(f\"‚úÖ Removed {test_duplicates} duplicates from test set\")\n",
    "else:\n",
    "    print(\"‚úÖ No duplicates found in test set\")\n",
    "\n",
    "print(f\"\\nFinal shapes:\")\n",
    "print(f\"Train: {train_clean.shape}\")\n",
    "print(f\"Test: {test_clean.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Basic Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (593994, 11)\n",
      "Training target shape: (593994,)\n",
      "Test features shape: (254569, 11)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X_train = train_clean.drop(['id', 'loan_paid_back'], axis=1)\n",
    "y_train = train_clean['loan_paid_back']\n",
    "X_test = test_clean.drop('id', axis=1)\n",
    "\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Training target shape: {y_train.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîß CREATING BASIC FEATURES\n",
      "============================================================\n",
      "‚úÖ Basic features created!\n",
      "New feature shape: (593994, 21)\n"
     ]
    }
   ],
   "source": [
    "# Create basic features\n",
    "print(\"=\" * 60)\n",
    "print(\"üîß CREATING BASIC FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"Create new features from existing ones\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Income to loan ratio (handle division by zero and NaN)\n",
    "    df['income_to_loan_ratio'] = df['annual_income'] / (df['loan_amount'] + 1)\n",
    "    df['income_to_loan_ratio'] = df['income_to_loan_ratio'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    # Monthly payment estimate (simplified)\n",
    "    df['monthly_payment_estimate'] = df['loan_amount'] * (df['interest_rate'] / 100 / 12)\n",
    "    df['monthly_payment_estimate'] = df['monthly_payment_estimate'].fillna(0)\n",
    "    \n",
    "    # Payment to income ratio (handle division by zero and NaN)\n",
    "    df['payment_to_income_ratio'] = df['monthly_payment_estimate'] / (df['annual_income'] / 12 + 1)\n",
    "    df['payment_to_income_ratio'] = df['payment_to_income_ratio'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    # Credit score categories (handle edge cases)\n",
    "    df['credit_score_category'] = pd.cut(\n",
    "        df['credit_score'], \n",
    "        bins=[0, 580, 670, 740, float('inf')], \n",
    "        labels=['Poor', 'Fair', 'Good', 'Excellent'],\n",
    "        include_lowest=True,\n",
    "        ordered=True\n",
    "    )\n",
    "    df['credit_score_category'] = df['credit_score_category'].fillna('Fair')\n",
    "    \n",
    "    # Interest rate categories\n",
    "    df['interest_rate_category'] = pd.cut(\n",
    "        df['interest_rate'],\n",
    "        bins=[0, 10, 13, 16, float('inf')],\n",
    "        labels=['Low', 'Medium', 'High', 'Very High'],\n",
    "        include_lowest=True,\n",
    "        ordered=True\n",
    "    )\n",
    "    df['interest_rate_category'] = df['interest_rate_category'].fillna('Medium')\n",
    "\n",
    "    # Loan amount categories\n",
    "    df['loan_amount_category'] = pd.cut(\n",
    "        df['loan_amount'],\n",
    "        bins=[0, 5000, 15000, 30000, float('inf')],\n",
    "        labels=['Small', 'Medium', 'Large', 'Very Large'],\n",
    "        include_lowest=True,\n",
    "        ordered=True\n",
    "    )\n",
    "    df['loan_amount_category'] = df['loan_amount_category'].fillna('Medium')\n",
    "    \n",
    "    # DTI risk level\n",
    "    df['dti_risk_level'] = pd.cut(\n",
    "        df['debt_to_income_ratio'],\n",
    "        bins=[0, 0.2, 0.4, 0.6, float('inf')],\n",
    "        labels=['Low', 'Medium', 'High', 'Very High'],\n",
    "        include_lowest=True,\n",
    "        ordered=True\n",
    "    )\n",
    "    df['dti_risk_level'] = df['dti_risk_level'].fillna('Medium')\n",
    "    \n",
    "    # Extract grade and subgrade from grade_subgrade\n",
    "    df['grade'] = df['grade_subgrade'].str[0]\n",
    "    df['subgrade'] = pd.to_numeric(df['grade_subgrade'].str[1:], errors='coerce').fillna(0).astype(int)\n",
    "    if 'grade_subgrade' in df.columns:\n",
    "        df = df.drop('grade_subgrade', axis=1)\n",
    "    \n",
    "    # Employment risk (Unemployed/Retired might be riskier)\n",
    "    df['employment_risk'] = df['employment_status'].map({\n",
    "        'Employed': 0,\n",
    "        'Self-employed': 1,\n",
    "        'Unemployed': 2,\n",
    "        'Retired': 1,\n",
    "        'Student': 1\n",
    "    })\n",
    "    df['employment_risk'] = df['employment_risk'].fillna(1)\n",
    "    \n",
    "    # Education level encoding (ordinal)\n",
    "    education_order = {\n",
    "        'High School': 1,\n",
    "        \"Bachelor's\": 2,\n",
    "        \"Master's\": 3,\n",
    "        'PhD': 4,\n",
    "        'Other': 0\n",
    "    }\n",
    "    df['education_encoded'] = df['education_level'].map(education_order)\n",
    "    df['education_encoded'] = df['education_encoded'].fillna(0)\n",
    "    \n",
    "    # Final check: replace any remaining NaN, inf, or -inf values in numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "X_train_fe = create_features(X_train)\n",
    "X_test_fe = create_features(X_test)\n",
    "\n",
    "print(\"‚úÖ Basic features created!\")\n",
    "print(f\"New feature shape: {X_train_fe.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Advanced Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîß ADVANCED FEATURE ENGINEERING\n",
      "============================================================\n",
      "‚úÖ Advanced features created:\n",
      "  - Interaction features (credit_score √ó interest_rate, etc.)\n",
      "  - Polynomial features (squared terms)\n",
      "  - Log transformations\n",
      "  - Risk and affordability scores\n",
      "\n",
      "New feature shape: (593994, 37)\n"
     ]
    }
   ],
   "source": [
    "# Advanced Feature Engineering - Part 2\n",
    "print(\"=\" * 60)\n",
    "print(\"üîß ADVANCED FEATURE ENGINEERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_advanced_features(df):\n",
    "    \"\"\"Create advanced interaction and polynomial features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Interaction features between important variables\n",
    "    df['credit_score_x_interest_rate'] = df['credit_score'] * df['interest_rate']\n",
    "    df['credit_score_x_dti'] = df['credit_score'] * df['debt_to_income_ratio']\n",
    "    df['income_x_credit_score'] = df['annual_income'] * df['credit_score']\n",
    "    df['loan_x_interest'] = df['loan_amount'] * df['interest_rate']\n",
    "    df['dti_x_interest'] = df['debt_to_income_ratio'] * df['interest_rate']\n",
    "    \n",
    "    # Ratio features\n",
    "    df['credit_to_loan_ratio'] = df['credit_score'] / (df['loan_amount'] + 1)\n",
    "    df['credit_to_income_ratio'] = df['credit_score'] / (df['annual_income'] + 1)\n",
    "    \n",
    "    # Polynomial features for key variables\n",
    "    df['credit_score_squared'] = df['credit_score'] ** 2\n",
    "    df['dti_squared'] = df['debt_to_income_ratio'] ** 2\n",
    "    df['interest_rate_squared'] = df['interest_rate'] ** 2\n",
    "    \n",
    "    # Log transformations (handle zeros)\n",
    "    df['log_annual_income'] = np.log1p(df['annual_income'])\n",
    "    df['log_loan_amount'] = np.log1p(df['loan_amount'])\n",
    "    df['log_credit_score'] = np.log1p(df['credit_score'])\n",
    "    \n",
    "    # Risk score combinations\n",
    "    df['risk_score'] = (df['debt_to_income_ratio'] * 0.4 + \n",
    "                       (1 - df['credit_score'] / 850) * 0.3 + \n",
    "                       (df['interest_rate'] / 30) * 0.3)\n",
    "    \n",
    "    # Affordability metrics\n",
    "    df['affordability_score'] = (df['annual_income'] / 12) / (df['monthly_payment_estimate'] + 1)\n",
    "    df['debt_service_ratio'] = df['debt_to_income_ratio'] * df['interest_rate'] / 100\n",
    "    \n",
    "    # Replace inf and NaN\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply advanced feature engineering\n",
    "X_train_advanced = create_advanced_features(X_train_fe)\n",
    "X_test_advanced = create_advanced_features(X_test_fe)\n",
    "\n",
    "print(\"‚úÖ Advanced features created:\")\n",
    "print(f\"  - Interaction features (credit_score √ó interest_rate, etc.)\")\n",
    "print(f\"  - Polynomial features (squared terms)\")\n",
    "print(f\"  - Log transformations\")\n",
    "print(f\"  - Risk and affordability scores\")\n",
    "print(f\"\\nNew feature shape: {X_train_advanced.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Categorical Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üè∑Ô∏è  RE-ENCODING WITH ADVANCED FEATURES\n",
      "============================================================\n",
      "‚úÖ Advanced features encoded and scaled!\n",
      "Final shape: (593994, 64)\n"
     ]
    }
   ],
   "source": [
    "# Re-encode categorical variables with new features\n",
    "print(\"=\" * 60)\n",
    "print(\"üè∑Ô∏è  RE-ENCODING WITH ADVANCED FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define nominal variables for one-hot encoding\n",
    "nominal_vars = ['gender', 'marital_status', 'education_level', 'employment_status', \n",
    "                'loan_purpose', 'grade', 'credit_score_category', \n",
    "                'interest_rate_category', 'loan_amount_category', 'dti_risk_level']\n",
    "\n",
    "# One-hot encode again with new features\n",
    "X_train_advanced_encoded = pd.get_dummies(X_train_advanced, columns=nominal_vars, \n",
    "                                          prefix=nominal_vars, drop_first=True)\n",
    "X_test_advanced_encoded = pd.get_dummies(X_test_advanced, columns=nominal_vars, \n",
    "                                         prefix=nominal_vars, drop_first=True)\n",
    "\n",
    "# Align columns\n",
    "missing_cols = set(X_train_advanced_encoded.columns) - set(X_test_advanced_encoded.columns)\n",
    "for col in missing_cols:\n",
    "    X_test_advanced_encoded[col] = 0\n",
    "X_test_advanced_encoded = X_test_advanced_encoded[X_train_advanced_encoded.columns]\n",
    "\n",
    "# Scale numerical features\n",
    "numerical_features_advanced = [col for col in X_train_advanced_encoded.columns \n",
    "                              if col not in [c for c in X_train_advanced_encoded.columns \n",
    "                                            if any(x in c for x in nominal_vars)]]\n",
    "numerical_features_advanced = [col for col in numerical_features_advanced \n",
    "                               if X_train_advanced_encoded[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "scaler_advanced = StandardScaler()\n",
    "X_train_advanced_scaled = X_train_advanced_encoded.copy()\n",
    "X_test_advanced_scaled = X_test_advanced_encoded.copy()\n",
    "\n",
    "X_train_advanced_scaled[numerical_features_advanced] = scaler_advanced.fit_transform(\n",
    "    X_train_advanced_encoded[numerical_features_advanced])\n",
    "X_test_advanced_scaled[numerical_features_advanced] = scaler_advanced.transform(\n",
    "    X_test_advanced_encoded[numerical_features_advanced])\n",
    "\n",
    "# Final cleanup\n",
    "X_train_advanced_scaled = X_train_advanced_scaled.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "X_test_advanced_scaled = X_test_advanced_scaled.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "print(f\"‚úÖ Advanced features encoded and scaled!\")\n",
    "print(f\"Final shape: {X_train_advanced_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üéØ FEATURE SELECTION\n",
      "============================================================\n",
      "Original features: 64\n",
      "Selected features: 48\n",
      "Features removed: 16\n",
      "\n",
      "‚úÖ Feature selection completed!\n",
      "Selected features shape: (593994, 48)\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection - Remove less important features\n",
    "print(\"=\" * 60)\n",
    "print(\"üéØ FEATURE SELECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use a quick model to get feature importance\n",
    "temp_model = xgb.XGBClassifier(n_estimators=50, random_state=42, eval_metric='logloss')\n",
    "temp_model.fit(X_train_advanced_scaled, y_train)\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': X_train_advanced_scaled.columns,\n",
    "    'importance': temp_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Select top features (keep top 90%)\n",
    "importance_threshold = feature_importance_df['importance'].quantile(0.1)  # Keep top 90%\n",
    "selected_features = feature_importance_df[feature_importance_df['importance'] > importance_threshold]['feature'].tolist()\n",
    "\n",
    "print(f\"Original features: {len(X_train_advanced_scaled.columns)}\")\n",
    "print(f\"Selected features: {len(selected_features)}\")\n",
    "print(f\"Features removed: {len(X_train_advanced_scaled.columns) - len(selected_features)}\")\n",
    "\n",
    "X_train_selected = X_train_advanced_scaled[selected_features]\n",
    "X_test_selected = X_test_advanced_scaled[selected_features]\n",
    "\n",
    "print(f\"\\n‚úÖ Feature selection completed!\")\n",
    "print(f\"Selected features shape: {X_train_selected.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Ensemble Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble training set: (475195, 48)\n",
      "Ensemble validation set: (118799, 48)\n"
     ]
    }
   ],
   "source": [
    "# Split data for ensemble training\n",
    "X_train_ens, X_val_ens, y_train_ens, y_val_ens = train_test_split(\n",
    "    X_train_selected, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Ensemble training set: {X_train_ens.shape}\")\n",
    "print(f\"Ensemble validation set: {X_val_ens.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ü§ñ TRAINING ENSEMBLE MODELS\n",
      "============================================================\n",
      "Training XGBoost...\n",
      "  XGBoost Val AUC: 0.9203\n",
      "Training LightGBM...\n",
      "  LightGBM Val AUC: 0.9202\n",
      "Training CatBoost...\n",
      "  CatBoost Val AUC: 0.9180\n",
      "\n",
      "‚úÖ Base models trained!\n"
     ]
    }
   ],
   "source": [
    "# Train multiple models for ensemble with class weights\n",
    "print(\"=\" * 60)\n",
    "print(\"ü§ñ TRAINING ENSEMBLE MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate class weights for imbalanced data\n",
    "class_weights = compute_sample_weight('balanced', y_train_ens)\n",
    "\n",
    "# Train base models with better hyperparameters\n",
    "base_models = {}\n",
    "\n",
    "# XGBoost with class weights\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.1,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1,\n",
    "    scale_pos_weight=len(y_train_ens[y_train_ens==0]) / len(y_train_ens[y_train_ens==1]),\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_model.fit(X_train_ens, y_train_ens, sample_weight=class_weights)\n",
    "base_models['XGBoost'] = xgb_model\n",
    "xgb_pred = xgb_model.predict_proba(X_val_ens)[:, 1]\n",
    "print(f\"  XGBoost Val AUC: {roc_auc_score(y_val_ens, xgb_pred):.4f}\")\n",
    "\n",
    "# LightGBM with class weights\n",
    "print(\"Training LightGBM...\")\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=50,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    min_child_samples=20,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1,\n",
    "    scale_pos_weight=len(y_train_ens[y_train_ens==0]) / len(y_train_ens[y_train_ens==1]),\n",
    "    random_state=42,\n",
    "    verbose=-1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lgb_model.fit(X_train_ens, y_train_ens, sample_weight=class_weights)\n",
    "base_models['LightGBM'] = lgb_model\n",
    "lgb_pred = lgb_model.predict_proba(X_val_ens)[:, 1]\n",
    "print(f\"  LightGBM Val AUC: {roc_auc_score(y_val_ens, lgb_pred):.4f}\")\n",
    "\n",
    "# CatBoost with class weights\n",
    "print(\"Training CatBoost...\")\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    depth=6,\n",
    "    learning_rate=0.05,\n",
    "    l2_leaf_reg=3,\n",
    "    subsample=0.9,\n",
    "    colsample_bylevel=0.9,\n",
    "    scale_pos_weight=len(y_train_ens[y_train_ens==0]) / len(y_train_ens[y_train_ens==1]),\n",
    "    random_state=42,\n",
    "    verbose=False,\n",
    "    thread_count=-1\n",
    ")\n",
    "cat_model.fit(X_train_ens, y_train_ens, sample_weight=class_weights)\n",
    "base_models['CatBoost'] = cat_model\n",
    "cat_pred = cat_model.predict_proba(X_val_ens)[:, 1]\n",
    "print(f\"  CatBoost Val AUC: {roc_auc_score(y_val_ens, cat_pred):.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Base models trained!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Stacking Ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìö CREATING STACKING ENSEMBLE\n",
      "============================================================\n",
      "Generating OOF predictions for XGBoost...\n",
      "Generating OOF predictions for LightGBM...\n",
      "Generating OOF predictions for CatBoost...\n",
      "\n",
      "Training meta-learner...\n",
      "‚úÖ Stacking Ensemble Val AUC: 0.9204\n",
      "‚úÖ Simple Average Val AUC: 0.9201\n",
      "\n",
      "üèÜ Using Stacking Ensemble (AUC: 0.9204)\n"
     ]
    }
   ],
   "source": [
    "# Create stacking ensemble using cross-validation\n",
    "print(\"=\" * 60)\n",
    "print(\"üìö CREATING STACKING ENSEMBLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate out-of-fold predictions for stacking\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "stack_train = np.zeros((X_train_ens.shape[0], len(base_models)))\n",
    "stack_val = np.zeros((X_val_ens.shape[0], len(base_models)))\n",
    "\n",
    "for idx, (name, model) in enumerate(base_models.items()):\n",
    "    print(f\"Generating OOF predictions for {name}...\")\n",
    "    # Out-of-fold predictions for training set\n",
    "    oof_preds = cross_val_predict(model, X_train_ens, y_train_ens, \n",
    "                                   cv=skf, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    stack_train[:, idx] = oof_preds\n",
    "    # Direct predictions for validation set\n",
    "    stack_val[:, idx] = model.predict_proba(X_val_ens)[:, 1]\n",
    "\n",
    "# Train meta-learner (Logistic Regression)\n",
    "print(\"\\nTraining meta-learner...\")\n",
    "meta_learner = LogisticRegression(random_state=42, max_iter=1000, C=0.1)\n",
    "meta_learner.fit(stack_train, y_train_ens)\n",
    "\n",
    "# Evaluate stacking ensemble\n",
    "stack_val_pred = meta_learner.predict_proba(stack_val)[:, 1]\n",
    "stack_auc = roc_auc_score(y_val_ens, stack_val_pred)\n",
    "print(f\"‚úÖ Stacking Ensemble Val AUC: {stack_auc:.4f}\")\n",
    "\n",
    "# Also try simple averaging\n",
    "avg_val_pred = np.mean(stack_val, axis=1)\n",
    "avg_auc = roc_auc_score(y_val_ens, avg_val_pred)\n",
    "print(f\"‚úÖ Simple Average Val AUC: {avg_auc:.4f}\")\n",
    "\n",
    "# Use the better method\n",
    "if stack_auc >= avg_auc:\n",
    "    use_stacking = True\n",
    "    print(f\"\\nüèÜ Using Stacking Ensemble (AUC: {stack_auc:.4f})\")\n",
    "else:\n",
    "    use_stacking = False\n",
    "    print(f\"\\nüèÜ Using Simple Average (AUC: {avg_auc:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Final Ensemble Training on Full Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üéØ TRAINING FINAL ENSEMBLE ON FULL DATA\n",
      "============================================================\n",
      "Retraining base models on full dataset...\n",
      "  Retraining XGBoost...\n",
      "  Retraining LightGBM...\n",
      "  Retraining CatBoost...\n",
      "\n",
      "Generating OOF predictions for meta-learner...\n",
      "  XGBoost...\n",
      "  LightGBM...\n",
      "  CatBoost...\n",
      "\n",
      "‚úÖ Final ensemble trained!\n",
      "‚úÖ Final Ensemble CV AUC: 0.9210 (+/- 0.0018)\n"
     ]
    }
   ],
   "source": [
    "# Train final ensemble on full dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"üéØ TRAINING FINAL ENSEMBLE ON FULL DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Retrain base models on full training data\n",
    "final_base_models = {}\n",
    "class_weights_full = compute_sample_weight('balanced', y_train)\n",
    "\n",
    "print(\"Retraining base models on full dataset...\")\n",
    "for name, model_template in base_models.items():\n",
    "    print(f\"  Retraining {name}...\")\n",
    "    if name == 'XGBoost':\n",
    "        final_model = xgb.XGBClassifier(\n",
    "            n_estimators=500, max_depth=6, learning_rate=0.05,\n",
    "            subsample=0.9, colsample_bytree=0.9, min_child_weight=3,\n",
    "            gamma=0.1, reg_alpha=0.1, reg_lambda=1,\n",
    "            scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),\n",
    "            random_state=42, eval_metric='logloss', n_jobs=-1\n",
    "        )\n",
    "    elif name == 'LightGBM':\n",
    "        final_model = lgb.LGBMClassifier(\n",
    "            n_estimators=500, max_depth=6, learning_rate=0.05,\n",
    "            num_leaves=50, subsample=0.9, colsample_bytree=0.9,\n",
    "            min_child_samples=20, reg_alpha=0.1, reg_lambda=1,\n",
    "            scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),\n",
    "            random_state=42, verbose=-1, n_jobs=-1\n",
    "        )\n",
    "    else:  # CatBoost\n",
    "        final_model = CatBoostClassifier(\n",
    "            iterations=500, depth=6, learning_rate=0.05,\n",
    "            l2_leaf_reg=3, subsample=0.9, colsample_bylevel=0.9,\n",
    "            scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),\n",
    "            random_state=42, verbose=False, thread_count=-1\n",
    "        )\n",
    "    \n",
    "    final_model.fit(X_train_selected, y_train, sample_weight=class_weights_full)\n",
    "    final_base_models[name] = final_model\n",
    "\n",
    "# Generate OOF predictions for meta-learner training\n",
    "print(\"\\nGenerating OOF predictions for meta-learner...\")\n",
    "stack_full_train = np.zeros((X_train_selected.shape[0], len(final_base_models)))\n",
    "\n",
    "for idx, (name, model) in enumerate(final_base_models.items()):\n",
    "    print(f\"  {name}...\")\n",
    "    oof_preds = cross_val_predict(model, X_train_selected, y_train, \n",
    "                                   cv=skf, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    stack_full_train[:, idx] = oof_preds\n",
    "\n",
    "# Train final meta-learner\n",
    "final_meta_learner = LogisticRegression(random_state=42, max_iter=1000, C=0.1)\n",
    "final_meta_learner.fit(stack_full_train, y_train)\n",
    "\n",
    "print(\"\\n‚úÖ Final ensemble trained!\")\n",
    "\n",
    "# Cross-validation score\n",
    "cv_scores_ensemble = cross_val_score(\n",
    "    final_meta_learner, stack_full_train, y_train, \n",
    "    cv=skf, scoring='roc_auc', n_jobs=-1\n",
    ")\n",
    "print(f\"‚úÖ Final Ensemble CV AUC: {cv_scores_ensemble.mean():.4f} (+/- {cv_scores_ensemble.std() * 2:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÆ Making Final Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîÆ MAKING FINAL PREDICTIONS\n",
      "============================================================\n",
      "Generating XGBoost predictions...\n",
      "Generating LightGBM predictions...\n",
      "Generating CatBoost predictions...\n",
      "Using stacking ensemble for final predictions\n",
      "\n",
      "Predictions shape: (254569,)\n",
      "Prediction range: [0.1106, 0.9890]\n",
      "Mean prediction: 0.6410\n",
      "\n",
      "‚úÖ Improved submission file created!\n",
      "Submission shape: (254569, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>loan_paid_back</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>593994</td>\n",
       "      <td>0.671066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>593995</td>\n",
       "      <td>0.954491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>593996</td>\n",
       "      <td>0.154719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>593997</td>\n",
       "      <td>0.660625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>593998</td>\n",
       "      <td>0.858155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>593999</td>\n",
       "      <td>0.933020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>594000</td>\n",
       "      <td>0.971704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>594001</td>\n",
       "      <td>0.915760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>594002</td>\n",
       "      <td>0.797269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>594003</td>\n",
       "      <td>0.110673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  loan_paid_back\n",
       "0  593994        0.671066\n",
       "1  593995        0.954491\n",
       "2  593996        0.154719\n",
       "3  593997        0.660625\n",
       "4  593998        0.858155\n",
       "5  593999        0.933020\n",
       "6  594000        0.971704\n",
       "7  594001        0.915760\n",
       "8  594002        0.797269\n",
       "9  594003        0.110673"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make final predictions\n",
    "print(\"=\" * 60)\n",
    "print(\"üîÆ MAKING FINAL PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate predictions from base models\n",
    "test_predictions_base = np.zeros((X_test_selected.shape[0], len(final_base_models)))\n",
    "\n",
    "for idx, (name, model) in enumerate(final_base_models.items()):\n",
    "    print(f\"Generating {name} predictions...\")\n",
    "    test_predictions_base[:, idx] = model.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Combine using meta-learner or simple average\n",
    "if use_stacking:\n",
    "    final_test_predictions = final_meta_learner.predict_proba(test_predictions_base)[:, 1]\n",
    "    print(\"Using stacking ensemble for final predictions\")\n",
    "else:\n",
    "    final_test_predictions = np.mean(test_predictions_base, axis=1)\n",
    "    print(\"Using simple average for final predictions\")\n",
    "\n",
    "print(f\"\\nPredictions shape: {final_test_predictions.shape}\")\n",
    "print(f\"Prediction range: [{final_test_predictions.min():.4f}, {final_test_predictions.max():.4f}]\")\n",
    "print(f\"Mean prediction: {final_test_predictions.mean():.4f}\")\n",
    "\n",
    "# Create submission file\n",
    "submission_improved = pd.DataFrame({\n",
    "    'id': test_clean['id'],\n",
    "    'loan_paid_back': final_test_predictions\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úÖ Improved submission file created!\")\n",
    "print(f\"Submission shape: {submission_improved.shape}\")\n",
    "submission_improved.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Improved submission saved to 'submission.csv'\n",
      "\n",
      "üìä Summary:\n",
      "  Final Ensemble CV AUC: 0.9210 (+/- 0.0018)\n",
      "  Expected improvement over baseline: +-0.0008\n"
     ]
    }
   ],
   "source": [
    "# Save improved submission\n",
    "submission_improved.to_csv('submission.csv', index=False)\n",
    "print(\"‚úÖ Improved submission saved to 'submission.csv'\")\n",
    "print(\"\\nüìä Summary:\")\n",
    "print(f\"  Final Ensemble CV AUC: {cv_scores_ensemble.mean():.4f} (+/- {cv_scores_ensemble.std() * 2:.4f})\")\n",
    "print(f\"  Expected improvement over baseline: +{cv_scores_ensemble.mean() - 0.92178:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
